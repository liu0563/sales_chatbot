{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e21b6e51-5b29-4b01-90c4-4b3bd9294a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.04 s (started: 2025-02-16 18:19:12 +08:00)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autotime\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "sys.path.append('/root/data_disk/src')\n",
    "\n",
    "from src.retriever import Retriever\n",
    "from src.LLM_utils import LLM\n",
    "from src.prompt import generate_prompt\n",
    "\n",
    "# import prompt\n",
    "# import importlib\n",
    "# importlib.reload(prompt)\n",
    "\n",
    "# from prompt import generate_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b73d983c-4ff6-4507-9e7a-cd7502b142d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model to directory: /root/autodl-tmp/.cache/modelscope/Qwen/Qwen2.5-3B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-16 18:20:23,441 - modelscope - INFO - Creating symbolic link [/root/autodl-tmp/.cache/modelscope/Qwen/Qwen2.5-3B-Instruct].\n",
      "2025-02-16 18:20:23,442 - modelscope - WARNING - Failed to create symbolic link /root/autodl-tmp/.cache/modelscope/Qwen/Qwen2.5-3B-Instruct for /root/autodl-tmp/.cache/modelscope/Qwen/Qwen2___5-3B-Instruct.\n",
      "[TM][WARNING] [LlamaTritonModel] `max_context_token_num` is not set, default to 32768.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-16 18:20:24,249 - lmdeploy - \u001b[33mWARNING\u001b[0m - turbomind.py:217 - get 327 model params\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] gemm_config.in is not found; using default GEMM algo\n",
      "time: 4.18 s (started: 2025-02-16 18:20:22 +08:00)\n"
     ]
    }
   ],
   "source": [
    "retriever = Retriever(embedder = 'BAAI/bge-m3',reranker = 'BAAI/bge-reranker-v2-m3',collection_name = 'sales_qa')\n",
    "model_id = 'Qwen/Qwen2.5-3B-Instruct'\n",
    "rewriter = LLM(model_id)\n",
    "\n",
    "model_id = 'ep-20250213200344-crq6r'\n",
    "gen_llm = LLM(model_id,api_key = os.getenv('ARK_API_KEY'),base_url = os.getenv('ARK_BASE_URL'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6ec7ac0-5bfb-4952-ba41-366f0b4b6daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 286 µs (started: 2025-02-16 18:20:27 +08:00)\n"
     ]
    }
   ],
   "source": [
    "past_convs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a811ac5-56d7-44af-8b32-496749f577a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-16 18:20:44,723 - lmdeploy - \u001b[33mWARNING\u001b[0m - async_engine.py:629 - GenerationConfig: GenerationConfig(n=1, max_new_tokens=512, do_sample=False, top_p=1.0, top_k=50, min_p=0.0, temperature=0.8, repetition_penalty=1.0, ignore_eos=False, random_seed=None, stop_words=None, bad_words=None, stop_token_ids=[151645], bad_token_ids=None, min_new_tokens=None, skip_special_tokens=True, spaces_between_special_tokens=True, logprobs=None, response_format=None, logits_processors=None, output_logits=None, output_last_hidden_state=None)\n",
      "2025-02-16 18:20:44,724 - lmdeploy - \u001b[33mWARNING\u001b[0m - async_engine.py:630 - Since v0.6.0, lmdeploy add `do_sample` in GenerationConfig. It defaults to False, meaning greedy decoding. Please set `do_sample=True` if sampling  decoding is needed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.68 s (started: 2025-02-16 18:20:44 +08:00)\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"I need a BMW car\"\"\"\n",
    "rewrite_prompt = generate_prompt('rewrite', query, past_convs, reranked_chunks=None)\n",
    "rewritten_query = rewriter(rewrite_prompt)\n",
    "past_convs.append(\"User: \" + rewritten_query)\n",
    "\n",
    "recall_docs, reranked_chunks = retriever.retrieve(rewritten_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "792b49c5-6ad7-4cea-8626-4dc63eb296aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting API request\n",
      "time: 8 s (started: 2025-02-16 18:20:47 +08:00)\n"
     ]
    }
   ],
   "source": [
    "generation_prompt = generate_prompt('generate', query, past_convs, reranked_chunks)\n",
    "response = gen_llm(generation_prompt)\n",
    "past_convs.append(\"Assistant: \" + response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaf6b130-9c5a-4346-b8cb-3612c8c52234",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: I need a BMW car\n",
      "Assistant: Based on your query, here are some BMW options that might suit your needs:\n",
      "\n",
      "1. **BMW X3 M**  \n",
      "   - A practical SUV with sports car-like performance. It’s fun to drive, has good visibility, and is comfortable for daily use. Fuel economy averages around 19 MPG, and it’s a great compromise if you’re used to sports cars or sedans.\n",
      "\n",
      "2. **BMW 4 Series 430i**  \n",
      "   - A stylish coupe with smooth and quiet performance, great for daily commutes. It offers excellent acceleration, a comfortable driver’s cockpit, and advanced tech features like voice command.\n",
      "\n",
      "3. **BMW M2**  \n",
      "   - A sporty coupe that delivers an ultimate driving experience. It’s fast, agile, and offers both manual and automatic transmission options. While the infotainment system has some drawbacks, it’s a fantastic choice for driving enthusiasts.\n",
      "\n",
      "4. **BMW Z4**  \n",
      "   - A fun-to-drive convertible roadster with quick acceleration and responsive handling. It’s a great value compared to competitors like the Porsche Boxster and offers a good balance of performance and practicality.\n",
      "\n",
      "Let me know if you’d like more details about any of these models!\n",
      "time: 894 µs (started: 2025-02-16 18:20:55 +08:00)\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\\n\"\"\".join(past_convs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "503230c8-07c7-4583-bfd0-253cf9de18d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "model = ChatCompletionsClient(\n",
    "    endpoint=os.getenv('AZURE_ENDPOINT'),\n",
    "    credential=AzureKeyCredential(os.getenv('AZURE_API_KEY')),\n",
    ")\n",
    "\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "\n",
    "response = model.complete(\n",
    "    messages=[\n",
    "        UserMessage(content='HI'),\n",
    "    ],\n",
    "    model=\"Llama-3.3-70B-Instruct\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4db39661-9704-4224-831b-4a3ada2156a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
