{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d12a7742-f896-4505-aa2e-d999f01470be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.4 s (started: 2025-02-15 10:08:49 +08:00)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "os.environ[\"HF_HOME\"] = \"/root/autodl-tmp/.cache/huggingface\"\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "import pandas as pd\n",
    "%load_ext autotime\n",
    "client = QdrantClient(path=\"./car/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84c2223f-dffa-4e93-8bd5-6d433f5fae4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 380 ms (started: 2025-02-15 10:08:52 +08:00)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c696742-7d29-4d29-9fb2-0315b4565919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 758 ms (started: 2025-02-15 10:08:54 +08:00)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "docs = []\n",
    "for file in os.listdir('./data/'):\n",
    "    #file not in ['porche.pkl','honda.pkl','bmw.pkl'] and \n",
    "    if file.endswith('.pkl'):\n",
    "        with open(f'./data/{file}', 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        for md_content in data:\n",
    "            markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[(\"#\", \"Header 1\"),(\"##\", \"Header 2\"),(\"###\", \"Header 3\"),(\"####\", \"Header 4\")])\n",
    "            md_header_splits = markdown_splitter.split_text(md_content)\n",
    "            md_chunks = [\", \".join(split.metadata.values()) + '\\n' + split.page_content for split in md_header_splits]\n",
    "            docs.extend(md_chunks)\n",
    "    # excludes = set('Header 1','CAUTION')\n",
    "    # md_header_splits = [split for split in md_header_splits if split.metadata['Header 1'] != 'WARNING']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96035bac-cf91-4fc1-a365-dcd918391ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362b64c968984ae980846593f73ae4ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pre tokenize: 100%|██████████| 160/160 [00:02<00:00, 65.17it/s]\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Inference Embeddings: 100%|██████████| 682/682 [01:00<00:00, 11.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 17s (started: 2025-02-15 10:08:58 +08:00)\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "model = BGEM3FlagModel('BAAI/bge-m3',  use_fp16=True) \n",
    "title_des_embeds = model.encode(docs,batch_size = 64,max_length = 8192,)['dense_vecs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28f35405-5989-41f6-bb64-73fd4a6d36dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 29.6 s (started: 2025-02-15 10:10:46 +08:00)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# NOTE: consider splitting the data into chunks to avoid hitting the server's payload size limit\n",
    "# or use `upload_collection` or `upload_points` methods which handle this for you\n",
    "# WARNING: uploading points one-by-one is not recommended due to requests overhead\n",
    "\n",
    "client.delete_collection('sales_qa')\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"sales_qa\",\n",
    "        vectors_config = {\n",
    "        'text': VectorParams(size=1024, distance=Distance.COSINE)  # Adjust vector size & metric\n",
    "        }\n",
    ")\n",
    "\n",
    "client.upsert(\n",
    "    collection_name=\"sales_qa\",\n",
    "    points=[\n",
    "        PointStruct(id=i, vector={\"text\": tit_des_embed.tolist() },\n",
    "        payload={'description':doc},\n",
    "        )\n",
    "        for i, (tit_des_embed, doc) in enumerate(zip(title_des_embeds, docs))\n",
    "    ] \n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
